{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghuram/anaconda3/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Current Total Reward: -10000.0, Running Total Reward: -10000.0\n",
      "Iteration: 1, Current Total Reward: -10000.0, Running Total Reward: -10000.0\n",
      "Iteration: 2, Current Total Reward: -10000.0, Running Total Reward: -10000.0\n",
      "Iteration: 3, Current Total Reward: -10000.0, Running Total Reward: -10000.0\n",
      "Iteration: 4, Current Total Reward: -4905.0, Running Total Reward: -9490.5\n",
      "Iteration: 5, Current Total Reward: -10000.0, Running Total Reward: -9541.45\n",
      "Iteration: 6, Current Total Reward: -10000.0, Running Total Reward: -9587.31\n",
      "Iteration: 7, Current Total Reward: -10000.0, Running Total Reward: -9628.57\n",
      "Iteration: 8, Current Total Reward: -8648.0, Running Total Reward: -9530.52\n",
      "Iteration: 9, Current Total Reward: -10000.0, Running Total Reward: -9577.47\n",
      "Iteration: 10, Current Total Reward: -10000.0, Running Total Reward: -9619.72\n",
      "Iteration: 11, Current Total Reward: -10000.0, Running Total Reward: -9657.75\n",
      "Iteration: 12, Current Total Reward: -2430.0, Running Total Reward: -8934.97\n",
      "Iteration: 13, Current Total Reward: -8764.0, Running Total Reward: -8917.88\n",
      "Iteration: 14, Current Total Reward: -6086.0, Running Total Reward: -8634.69\n",
      "Iteration: 15, Current Total Reward: -5484.0, Running Total Reward: -8319.62\n",
      "Iteration: 16, Current Total Reward: -3262.0, Running Total Reward: -7813.86\n",
      "Iteration: 17, Current Total Reward: -4415.0, Running Total Reward: -7473.97\n",
      "Iteration: 18, Current Total Reward: -5773.0, Running Total Reward: -7303.87\n",
      "Iteration: 19, Current Total Reward: -950.0, Running Total Reward: -6668.49\n",
      "Iteration: 20, Current Total Reward: -2258.0, Running Total Reward: -6227.44\n",
      "Iteration: 21, Current Total Reward: -2204.0, Running Total Reward: -5825.09\n",
      "Iteration: 22, Current Total Reward: -2149.0, Running Total Reward: -5457.48\n",
      "Iteration: 23, Current Total Reward: -1698.0, Running Total Reward: -5081.54\n",
      "Iteration: 24, Current Total Reward: -1507.0, Running Total Reward: -4724.08\n",
      "Iteration: 25, Current Total Reward: -1382.0, Running Total Reward: -4389.87\n",
      "Iteration: 26, Current Total Reward: -1191.0, Running Total Reward: -4069.99\n",
      "Iteration: 27, Current Total Reward: -682.0, Running Total Reward: -3731.19\n",
      "Iteration: 28, Current Total Reward: -572.0, Running Total Reward: -3415.27\n",
      "Iteration: 29, Current Total Reward: -552.0, Running Total Reward: -3128.94\n",
      "Iteration: 30, Current Total Reward: -555.0, Running Total Reward: -2871.55\n",
      "Iteration: 31, Current Total Reward: -535.0, Running Total Reward: -2637.89\n",
      "Iteration: 32, Current Total Reward: -451.0, Running Total Reward: -2419.2\n",
      "Iteration: 33, Current Total Reward: -353.0, Running Total Reward: -2212.58\n",
      "Iteration: 34, Current Total Reward: -340.0, Running Total Reward: -2025.33\n",
      "Iteration: 35, Current Total Reward: -468.0, Running Total Reward: -1869.59\n",
      "Iteration: 36, Current Total Reward: -264.0, Running Total Reward: -1709.03\n",
      "Iteration: 37, Current Total Reward: -261.0, Running Total Reward: -1564.23\n",
      "Iteration: 38, Current Total Reward: -190.0, Running Total Reward: -1426.81\n",
      "Iteration: 39, Current Total Reward: -297.0, Running Total Reward: -1313.83\n",
      "Iteration: 40, Current Total Reward: -162.0, Running Total Reward: -1198.64\n",
      "Iteration: 41, Current Total Reward: -185.0, Running Total Reward: -1097.28\n",
      "Iteration: 42, Current Total Reward: -192.0, Running Total Reward: -1006.75\n",
      "Iteration: 43, Current Total Reward: -200.0, Running Total Reward: -926.08\n",
      "Iteration: 44, Current Total Reward: -149.0, Running Total Reward: -848.37\n",
      "Iteration: 45, Current Total Reward: -172.0, Running Total Reward: -780.73\n",
      "Iteration: 46, Current Total Reward: -141.0, Running Total Reward: -716.76\n",
      "Iteration: 47, Current Total Reward: -158.0, Running Total Reward: -660.88\n",
      "Iteration: 48, Current Total Reward: -179.0, Running Total Reward: -612.69\n",
      "Iteration: 49, Current Total Reward: -159.0, Running Total Reward: -567.33\n",
      "Iteration: 50, Current Total Reward: -204.0, Running Total Reward: -530.99\n",
      "Iteration: 51, Current Total Reward: -135.0, Running Total Reward: -491.39\n",
      "Iteration: 52, Current Total Reward: -150.0, Running Total Reward: -457.25\n",
      "Iteration: 53, Current Total Reward: -133.0, Running Total Reward: -424.83\n",
      "Iteration: 54, Current Total Reward: -154.0, Running Total Reward: -397.75\n",
      "Iteration: 55, Current Total Reward: -119.0, Running Total Reward: -369.87\n",
      "Iteration: 56, Current Total Reward: -111.0, Running Total Reward: -343.98\n",
      "Iteration: 57, Current Total Reward: -153.0, Running Total Reward: -324.89\n",
      "Iteration: 58, Current Total Reward: -159.0, Running Total Reward: -308.3\n",
      "Iteration: 59, Current Total Reward: -138.0, Running Total Reward: -291.27\n",
      "Iteration: 60, Current Total Reward: -156.0, Running Total Reward: -277.74\n",
      "Iteration: 61, Current Total Reward: -159.0, Running Total Reward: -265.87\n",
      "Iteration: 62, Current Total Reward: -162.0, Running Total Reward: -255.48\n",
      "Iteration: 63, Current Total Reward: -159.0, Running Total Reward: -245.83\n",
      "Iteration: 64, Current Total Reward: -164.0, Running Total Reward: -237.65\n",
      "Iteration: 65, Current Total Reward: -138.0, Running Total Reward: -227.68\n",
      "Iteration: 66, Current Total Reward: -160.0, Running Total Reward: -220.92\n",
      "Iteration: 67, Current Total Reward: -155.0, Running Total Reward: -214.32\n",
      "Iteration: 68, Current Total Reward: -189.0, Running Total Reward: -211.79\n",
      "Iteration: 69, Current Total Reward: -153.0, Running Total Reward: -205.91\n",
      "Iteration: 70, Current Total Reward: -183.0, Running Total Reward: -203.62\n",
      "Iteration: 71, Current Total Reward: -164.0, Running Total Reward: -199.66\n",
      "Iteration: 72, Current Total Reward: -161.0, Running Total Reward: -195.79\n",
      "Iteration: 73, Current Total Reward: -151.0, Running Total Reward: -191.31\n",
      "Iteration: 74, Current Total Reward: -126.0, Running Total Reward: -184.78\n",
      "Iteration: 75, Current Total Reward: -119.0, Running Total Reward: -178.2\n",
      "Iteration: 76, Current Total Reward: -154.0, Running Total Reward: -175.78\n",
      "Iteration: 77, Current Total Reward: -89.0, Running Total Reward: -167.11\n",
      "Iteration: 78, Current Total Reward: -89.0, Running Total Reward: -159.29\n",
      "Iteration: 79, Current Total Reward: -92.0, Running Total Reward: -152.57\n",
      "Iteration: 80, Current Total Reward: -94.0, Running Total Reward: -146.71\n",
      "Iteration: 81, Current Total Reward: -127.0, Running Total Reward: -144.74\n",
      "Iteration: 82, Current Total Reward: -151.0, Running Total Reward: -145.36\n",
      "Iteration: 83, Current Total Reward: -177.0, Running Total Reward: -148.53\n",
      "Iteration: 84, Current Total Reward: -155.0, Running Total Reward: -149.17\n",
      "Iteration: 85, Current Total Reward: -88.0, Running Total Reward: -143.06\n",
      "Iteration: 86, Current Total Reward: -150.0, Running Total Reward: -143.75\n",
      "Iteration: 87, Current Total Reward: -86.0, Running Total Reward: -137.98\n",
      "Iteration: 88, Current Total Reward: -118.0, Running Total Reward: -135.98\n",
      "Iteration: 89, Current Total Reward: -98.0, Running Total Reward: -132.18\n",
      "Iteration: 90, Current Total Reward: -157.0, Running Total Reward: -134.66\n",
      "Iteration: 91, Current Total Reward: -156.0, Running Total Reward: -136.8\n",
      "Iteration: 92, Current Total Reward: -155.0, Running Total Reward: -138.62\n",
      "Iteration: 93, Current Total Reward: -164.0, Running Total Reward: -141.16\n",
      "Iteration: 94, Current Total Reward: -90.0, Running Total Reward: -136.04\n",
      "Iteration: 95, Current Total Reward: -183.0, Running Total Reward: -140.74\n",
      "Iteration: 96, Current Total Reward: -156.0, Running Total Reward: -142.26\n",
      "Iteration: 97, Current Total Reward: -158.0, Running Total Reward: -143.84\n",
      "Iteration: 98, Current Total Reward: -153.0, Running Total Reward: -144.75\n",
      "Iteration: 99, Current Total Reward: -107.0, Running Total Reward: -140.98\n",
      "Iteration: 100, Current Total Reward: -151.0, Running Total Reward: -141.98\n",
      "Iteration: 101, Current Total Reward: -157.0, Running Total Reward: -143.48\n",
      "Iteration: 102, Current Total Reward: -172.0, Running Total Reward: -146.33\n",
      "Iteration: 103, Current Total Reward: -128.0, Running Total Reward: -144.5\n",
      "Iteration: 104, Current Total Reward: -93.0, Running Total Reward: -139.35\n",
      "Iteration: 105, Current Total Reward: -153.0, Running Total Reward: -140.71\n",
      "Iteration: 106, Current Total Reward: -92.0, Running Total Reward: -135.84\n",
      "Iteration: 107, Current Total Reward: -143.0, Running Total Reward: -136.56\n",
      "Iteration: 108, Current Total Reward: -95.0, Running Total Reward: -132.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 109, Current Total Reward: -188.0, Running Total Reward: -137.96\n",
      "Iteration: 110, Current Total Reward: -162.0, Running Total Reward: -140.37\n",
      "Iteration: 111, Current Total Reward: -149.0, Running Total Reward: -141.23\n",
      "Iteration: 112, Current Total Reward: -163.0, Running Total Reward: -143.41\n",
      "Iteration: 113, Current Total Reward: -154.0, Running Total Reward: -144.47\n",
      "Iteration: 114, Current Total Reward: -150.0, Running Total Reward: -145.02\n",
      "Iteration: 115, Current Total Reward: -90.0, Running Total Reward: -139.52\n",
      "Iteration: 116, Current Total Reward: -87.0, Running Total Reward: -134.27\n",
      "Iteration: 117, Current Total Reward: -148.0, Running Total Reward: -135.64\n",
      "Iteration: 118, Current Total Reward: -156.0, Running Total Reward: -137.68\n",
      "Iteration: 119, Current Total Reward: -157.0, Running Total Reward: -139.61\n",
      "Iteration: 120, Current Total Reward: -95.0, Running Total Reward: -135.15\n",
      "Iteration: 121, Current Total Reward: -164.0, Running Total Reward: -138.03\n",
      "Iteration: 122, Current Total Reward: -84.0, Running Total Reward: -132.63\n",
      "Iteration: 123, Current Total Reward: -155.0, Running Total Reward: -134.87\n",
      "Iteration: 124, Current Total Reward: -153.0, Running Total Reward: -136.68\n",
      "Iteration: 125, Current Total Reward: -159.0, Running Total Reward: -138.91\n",
      "Iteration: 126, Current Total Reward: -154.0, Running Total Reward: -140.42\n",
      "Iteration: 127, Current Total Reward: -151.0, Running Total Reward: -141.48\n",
      "Iteration: 128, Current Total Reward: -124.0, Running Total Reward: -139.73\n",
      "Iteration: 129, Current Total Reward: -89.0, Running Total Reward: -134.66\n",
      "Iteration: 130, Current Total Reward: -157.0, Running Total Reward: -136.89\n",
      "Iteration: 131, Current Total Reward: -158.0, Running Total Reward: -139.0\n",
      "Iteration: 132, Current Total Reward: -156.0, Running Total Reward: -140.7\n",
      "Iteration: 133, Current Total Reward: -99.0, Running Total Reward: -136.53\n",
      "Iteration: 134, Current Total Reward: -92.0, Running Total Reward: -132.08\n",
      "Iteration: 135, Current Total Reward: -146.0, Running Total Reward: -133.47\n",
      "Iteration: 136, Current Total Reward: -96.0, Running Total Reward: -129.72\n",
      "Iteration: 137, Current Total Reward: -151.0, Running Total Reward: -131.85\n",
      "Iteration: 138, Current Total Reward: -158.0, Running Total Reward: -134.47\n",
      "Iteration: 139, Current Total Reward: -86.0, Running Total Reward: -129.62\n",
      "Iteration: 140, Current Total Reward: -168.0, Running Total Reward: -133.46\n",
      "Iteration: 141, Current Total Reward: -156.0, Running Total Reward: -135.71\n",
      "Iteration: 142, Current Total Reward: -145.0, Running Total Reward: -136.64\n",
      "Iteration: 143, Current Total Reward: -156.0, Running Total Reward: -138.58\n",
      "Iteration: 144, Current Total Reward: -89.0, Running Total Reward: -133.62\n",
      "Iteration: 145, Current Total Reward: -153.0, Running Total Reward: -135.56\n",
      "Iteration: 146, Current Total Reward: -87.0, Running Total Reward: -130.7\n",
      "Iteration: 147, Current Total Reward: -123.0, Running Total Reward: -129.93\n",
      "Iteration: 148, Current Total Reward: -157.0, Running Total Reward: -132.64\n",
      "Iteration: 149, Current Total Reward: -174.0, Running Total Reward: -136.77\n",
      "Iteration: 150, Current Total Reward: -157.0, Running Total Reward: -138.8\n",
      "Iteration: 151, Current Total Reward: -88.0, Running Total Reward: -133.72\n",
      "Iteration: 152, Current Total Reward: -158.0, Running Total Reward: -136.15\n",
      "Iteration: 153, Current Total Reward: -159.0, Running Total Reward: -138.43\n",
      "Iteration: 154, Current Total Reward: -153.0, Running Total Reward: -139.89\n",
      "Iteration: 155, Current Total Reward: -157.0, Running Total Reward: -141.6\n",
      "Iteration: 156, Current Total Reward: -149.0, Running Total Reward: -142.34\n",
      "Iteration: 157, Current Total Reward: -194.0, Running Total Reward: -147.51\n",
      "Iteration: 158, Current Total Reward: -156.0, Running Total Reward: -148.35\n",
      "Iteration: 159, Current Total Reward: -185.0, Running Total Reward: -152.02\n",
      "Iteration: 160, Current Total Reward: -161.0, Running Total Reward: -152.92\n",
      "Iteration: 161, Current Total Reward: -88.0, Running Total Reward: -146.43\n",
      "Iteration: 162, Current Total Reward: -95.0, Running Total Reward: -141.28\n",
      "Iteration: 163, Current Total Reward: -152.0, Running Total Reward: -142.35\n",
      "Iteration: 164, Current Total Reward: -159.0, Running Total Reward: -144.02\n",
      "Iteration: 165, Current Total Reward: -152.0, Running Total Reward: -144.82\n",
      "Iteration: 166, Current Total Reward: -162.0, Running Total Reward: -146.54\n",
      "Iteration: 167, Current Total Reward: -175.0, Running Total Reward: -149.38\n",
      "Iteration: 168, Current Total Reward: -103.0, Running Total Reward: -144.74\n",
      "Iteration: 169, Current Total Reward: -170.0, Running Total Reward: -147.27\n",
      "Iteration: 170, Current Total Reward: -170.0, Running Total Reward: -149.54\n",
      "Iteration: 171, Current Total Reward: -96.0, Running Total Reward: -144.19\n",
      "Iteration: 172, Current Total Reward: -85.0, Running Total Reward: -138.27\n",
      "Iteration: 173, Current Total Reward: -158.0, Running Total Reward: -140.24\n",
      "Iteration: 174, Current Total Reward: -87.0, Running Total Reward: -134.92\n",
      "Iteration: 175, Current Total Reward: -157.0, Running Total Reward: -137.13\n",
      "Iteration: 176, Current Total Reward: -154.0, Running Total Reward: -138.81\n",
      "Iteration: 177, Current Total Reward: -96.0, Running Total Reward: -134.53\n",
      "Iteration: 178, Current Total Reward: -149.0, Running Total Reward: -135.98\n",
      "Iteration: 179, Current Total Reward: -156.0, Running Total Reward: -137.98\n",
      "Iteration: 180, Current Total Reward: -179.0, Running Total Reward: -142.08\n",
      "Iteration: 181, Current Total Reward: -98.0, Running Total Reward: -137.67\n",
      "Iteration: 182, Current Total Reward: -161.0, Running Total Reward: -140.01\n",
      "Iteration: 183, Current Total Reward: -157.0, Running Total Reward: -141.71\n",
      "Iteration: 184, Current Total Reward: -86.0, Running Total Reward: -136.14\n",
      "Iteration: 185, Current Total Reward: -140.0, Running Total Reward: -136.52\n",
      "Iteration: 186, Current Total Reward: -133.0, Running Total Reward: -136.17\n",
      "Iteration: 187, Current Total Reward: -152.0, Running Total Reward: -137.75\n",
      "Iteration: 188, Current Total Reward: -152.0, Running Total Reward: -139.18\n",
      "Iteration: 189, Current Total Reward: -88.0, Running Total Reward: -134.06\n",
      "Iteration: 190, Current Total Reward: -215.0, Running Total Reward: -142.15\n",
      "Iteration: 191, Current Total Reward: -88.0, Running Total Reward: -136.74\n",
      "Iteration: 192, Current Total Reward: -95.0, Running Total Reward: -132.56\n",
      "Iteration: 193, Current Total Reward: -93.0, Running Total Reward: -128.61\n",
      "Iteration: 194, Current Total Reward: -93.0, Running Total Reward: -125.05\n",
      "Iteration: 195, Current Total Reward: -157.0, Running Total Reward: -128.24\n",
      "Iteration: 196, Current Total Reward: -153.0, Running Total Reward: -130.72\n",
      "Iteration: 197, Current Total Reward: -158.0, Running Total Reward: -133.45\n",
      "Iteration: 198, Current Total Reward: -144.0, Running Total Reward: -134.5\n",
      "Iteration: 199, Current Total Reward: -167.0, Running Total Reward: -137.75\n",
      "Iteration: 200, Current Total Reward: -154.0, Running Total Reward: -139.38\n",
      "Iteration: 201, Current Total Reward: -163.0, Running Total Reward: -141.74\n",
      "Iteration: 202, Current Total Reward: -163.0, Running Total Reward: -143.87\n",
      "Iteration: 203, Current Total Reward: -165.0, Running Total Reward: -145.98\n",
      "Iteration: 204, Current Total Reward: -92.0, Running Total Reward: -140.58\n",
      "Iteration: 205, Current Total Reward: -88.0, Running Total Reward: -135.32\n",
      "Iteration: 206, Current Total Reward: -160.0, Running Total Reward: -137.79\n",
      "Iteration: 207, Current Total Reward: -90.0, Running Total Reward: -133.01\n",
      "Iteration: 208, Current Total Reward: -88.0, Running Total Reward: -128.51\n",
      "Iteration: 209, Current Total Reward: -156.0, Running Total Reward: -131.26\n",
      "Iteration: 210, Current Total Reward: -92.0, Running Total Reward: -127.33\n",
      "Iteration: 211, Current Total Reward: -95.0, Running Total Reward: -124.1\n",
      "Iteration: 212, Current Total Reward: -143.0, Running Total Reward: -125.99\n",
      "Iteration: 213, Current Total Reward: -90.0, Running Total Reward: -122.39\n",
      "Iteration: 214, Current Total Reward: -152.0, Running Total Reward: -125.35\n",
      "Iteration: 215, Current Total Reward: -176.0, Running Total Reward: -130.42\n",
      "Iteration: 216, Current Total Reward: -150.0, Running Total Reward: -132.37\n",
      "Iteration: 217, Current Total Reward: -85.0, Running Total Reward: -127.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 218, Current Total Reward: -159.0, Running Total Reward: -130.77\n",
      "Iteration: 219, Current Total Reward: -89.0, Running Total Reward: -126.6\n",
      "Iteration: 220, Current Total Reward: -152.0, Running Total Reward: -129.14\n",
      "Iteration: 221, Current Total Reward: -155.0, Running Total Reward: -131.72\n",
      "Iteration: 222, Current Total Reward: -148.0, Running Total Reward: -133.35\n",
      "Iteration: 223, Current Total Reward: -161.0, Running Total Reward: -136.12\n",
      "Iteration: 224, Current Total Reward: -161.0, Running Total Reward: -138.6\n",
      "Iteration: 225, Current Total Reward: -88.0, Running Total Reward: -133.54\n",
      "Iteration: 226, Current Total Reward: -89.0, Running Total Reward: -129.09\n",
      "Iteration: 227, Current Total Reward: -88.0, Running Total Reward: -124.98\n",
      "Iteration: 228, Current Total Reward: -152.0, Running Total Reward: -127.68\n",
      "Iteration: 229, Current Total Reward: -157.0, Running Total Reward: -130.61\n",
      "Iteration: 230, Current Total Reward: -150.0, Running Total Reward: -132.55\n",
      "Iteration: 231, Current Total Reward: -154.0, Running Total Reward: -134.7\n",
      "Iteration: 232, Current Total Reward: -97.0, Running Total Reward: -130.93\n",
      "Iteration: 233, Current Total Reward: -160.0, Running Total Reward: -133.83\n",
      "Iteration: 234, Current Total Reward: -91.0, Running Total Reward: -129.55\n",
      "Iteration: 235, Current Total Reward: -156.0, Running Total Reward: -132.2\n",
      "Iteration: 236, Current Total Reward: -165.0, Running Total Reward: -135.48\n",
      "Iteration: 237, Current Total Reward: -160.0, Running Total Reward: -137.93\n",
      "Iteration: 238, Current Total Reward: -87.0, Running Total Reward: -132.84\n",
      "Iteration: 239, Current Total Reward: -163.0, Running Total Reward: -135.85\n",
      "Iteration: 240, Current Total Reward: -158.0, Running Total Reward: -138.07\n",
      "Iteration: 241, Current Total Reward: -90.0, Running Total Reward: -133.26\n",
      "Iteration: 242, Current Total Reward: -146.0, Running Total Reward: -134.53\n",
      "Iteration: 243, Current Total Reward: -87.0, Running Total Reward: -129.78\n",
      "Iteration: 244, Current Total Reward: -96.0, Running Total Reward: -126.4\n",
      "Iteration: 245, Current Total Reward: -178.0, Running Total Reward: -131.56\n",
      "Iteration: 246, Current Total Reward: -180.0, Running Total Reward: -136.41\n",
      "Iteration: 247, Current Total Reward: -152.0, Running Total Reward: -137.97\n",
      "Iteration: 248, Current Total Reward: -151.0, Running Total Reward: -139.27\n",
      "Iteration: 249, Current Total Reward: -155.0, Running Total Reward: -140.84\n",
      "Iteration: 250, Current Total Reward: -129.0, Running Total Reward: -139.66\n",
      "Iteration: 251, Current Total Reward: -102.0, Running Total Reward: -135.89\n",
      "Iteration: 252, Current Total Reward: -154.0, Running Total Reward: -137.7\n",
      "Iteration: 253, Current Total Reward: -91.0, Running Total Reward: -133.03\n",
      "Iteration: 254, Current Total Reward: -165.0, Running Total Reward: -136.23\n",
      "Iteration: 255, Current Total Reward: -85.0, Running Total Reward: -131.11\n",
      "Iteration: 256, Current Total Reward: -167.0, Running Total Reward: -134.7\n",
      "Iteration: 257, Current Total Reward: -106.0, Running Total Reward: -131.83\n",
      "Iteration: 258, Current Total Reward: -128.0, Running Total Reward: -131.44\n",
      "Iteration: 259, Current Total Reward: -155.0, Running Total Reward: -133.8\n",
      "Iteration: 260, Current Total Reward: -99.0, Running Total Reward: -130.32\n",
      "Iteration: 261, Current Total Reward: -86.0, Running Total Reward: -125.89\n",
      "Iteration: 262, Current Total Reward: -109.0, Running Total Reward: -124.2\n",
      "Iteration: 263, Current Total Reward: -97.0, Running Total Reward: -121.48\n",
      "Iteration: 264, Current Total Reward: -168.0, Running Total Reward: -126.13\n",
      "Iteration: 265, Current Total Reward: -87.0, Running Total Reward: -122.22\n",
      "Iteration: 266, Current Total Reward: -169.0, Running Total Reward: -126.9\n",
      "Iteration: 267, Current Total Reward: -87.0, Running Total Reward: -122.91\n",
      "Iteration: 268, Current Total Reward: -152.0, Running Total Reward: -125.82\n",
      "Iteration: 269, Current Total Reward: -147.0, Running Total Reward: -127.93\n",
      "Iteration: 270, Current Total Reward: -155.0, Running Total Reward: -130.64\n",
      "Iteration: 271, Current Total Reward: -93.0, Running Total Reward: -126.88\n",
      "Iteration: 272, Current Total Reward: -162.0, Running Total Reward: -130.39\n",
      "Iteration: 273, Current Total Reward: -87.0, Running Total Reward: -126.05\n",
      "Iteration: 274, Current Total Reward: -89.0, Running Total Reward: -122.35\n",
      "Iteration: 275, Current Total Reward: -96.0, Running Total Reward: -119.71\n",
      "Iteration: 276, Current Total Reward: -163.0, Running Total Reward: -124.04\n",
      "Iteration: 277, Current Total Reward: -145.0, Running Total Reward: -126.14\n",
      "Iteration: 278, Current Total Reward: -86.0, Running Total Reward: -122.12\n",
      "Iteration: 279, Current Total Reward: -157.0, Running Total Reward: -125.61\n",
      "Iteration: 280, Current Total Reward: -179.0, Running Total Reward: -130.95\n",
      "Iteration: 281, Current Total Reward: -131.0, Running Total Reward: -130.95\n",
      "Iteration: 282, Current Total Reward: -163.0, Running Total Reward: -134.16\n",
      "Iteration: 283, Current Total Reward: -88.0, Running Total Reward: -129.54\n",
      "Iteration: 284, Current Total Reward: -100.0, Running Total Reward: -126.59\n",
      "Iteration: 285, Current Total Reward: -149.0, Running Total Reward: -128.83\n",
      "Iteration: 286, Current Total Reward: -91.0, Running Total Reward: -125.05\n",
      "Iteration: 287, Current Total Reward: -159.0, Running Total Reward: -128.44\n",
      "Iteration: 288, Current Total Reward: -159.0, Running Total Reward: -131.5\n",
      "Iteration: 289, Current Total Reward: -157.0, Running Total Reward: -134.05\n",
      "Iteration: 290, Current Total Reward: -85.0, Running Total Reward: -129.14\n",
      "Iteration: 291, Current Total Reward: -163.0, Running Total Reward: -132.53\n",
      "Iteration: 292, Current Total Reward: -149.0, Running Total Reward: -134.18\n",
      "Iteration: 293, Current Total Reward: -91.0, Running Total Reward: -129.86\n",
      "Iteration: 294, Current Total Reward: -161.0, Running Total Reward: -132.97\n",
      "Iteration: 295, Current Total Reward: -87.0, Running Total Reward: -128.38\n",
      "Iteration: 296, Current Total Reward: -152.0, Running Total Reward: -130.74\n",
      "Iteration: 297, Current Total Reward: -161.0, Running Total Reward: -133.76\n",
      "Iteration: 298, Current Total Reward: -128.0, Running Total Reward: -133.19\n",
      "Iteration: 299, Current Total Reward: -88.0, Running Total Reward: -128.67\n",
      "Iteration: 300, Current Total Reward: -105.0, Running Total Reward: -126.3\n",
      "Iteration: 301, Current Total Reward: -99.0, Running Total Reward: -123.57\n",
      "Iteration: 302, Current Total Reward: -164.0, Running Total Reward: -127.61\n",
      "Iteration: 303, Current Total Reward: -90.0, Running Total Reward: -123.85\n",
      "Iteration: 304, Current Total Reward: -170.0, Running Total Reward: -128.47\n",
      "Iteration: 305, Current Total Reward: -150.0, Running Total Reward: -130.62\n",
      "Iteration: 306, Current Total Reward: -108.0, Running Total Reward: -128.36\n",
      "Iteration: 307, Current Total Reward: -155.0, Running Total Reward: -131.02\n",
      "Iteration: 308, Current Total Reward: -166.0, Running Total Reward: -134.52\n",
      "Iteration: 309, Current Total Reward: -165.0, Running Total Reward: -137.57\n",
      "Iteration: 310, Current Total Reward: -157.0, Running Total Reward: -139.51\n",
      "Iteration: 311, Current Total Reward: -152.0, Running Total Reward: -140.76\n",
      "Iteration: 312, Current Total Reward: -161.0, Running Total Reward: -142.78\n",
      "Iteration: 313, Current Total Reward: -158.0, Running Total Reward: -144.31\n",
      "Iteration: 314, Current Total Reward: -94.0, Running Total Reward: -139.28\n",
      "Iteration: 315, Current Total Reward: -92.0, Running Total Reward: -134.55\n",
      "Iteration: 316, Current Total Reward: -146.0, Running Total Reward: -135.69\n",
      "Iteration: 317, Current Total Reward: -154.0, Running Total Reward: -137.52\n",
      "Iteration: 318, Current Total Reward: -90.0, Running Total Reward: -132.77\n",
      "Iteration: 319, Current Total Reward: -91.0, Running Total Reward: -128.59\n",
      "Iteration: 320, Current Total Reward: -155.0, Running Total Reward: -131.23\n",
      "Iteration: 321, Current Total Reward: -149.0, Running Total Reward: -133.01\n",
      "Iteration: 322, Current Total Reward: -160.0, Running Total Reward: -135.71\n",
      "Iteration: 323, Current Total Reward: -152.0, Running Total Reward: -137.34\n",
      "Iteration: 324, Current Total Reward: -154.0, Running Total Reward: -139.01\n",
      "Iteration: 325, Current Total Reward: -87.0, Running Total Reward: -133.8\n",
      "Iteration: 326, Current Total Reward: -157.0, Running Total Reward: -136.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 327, Current Total Reward: -89.0, Running Total Reward: -131.41\n",
      "Iteration: 328, Current Total Reward: -87.0, Running Total Reward: -126.97\n",
      "Iteration: 329, Current Total Reward: -153.0, Running Total Reward: -129.57\n",
      "Iteration: 330, Current Total Reward: -95.0, Running Total Reward: -126.12\n",
      "Iteration: 331, Current Total Reward: -164.0, Running Total Reward: -129.9\n",
      "Iteration: 332, Current Total Reward: -92.0, Running Total Reward: -126.11\n",
      "Iteration: 333, Current Total Reward: -158.0, Running Total Reward: -129.3\n",
      "Iteration: 334, Current Total Reward: -139.0, Running Total Reward: -130.27\n",
      "Iteration: 335, Current Total Reward: -162.0, Running Total Reward: -133.45\n",
      "Iteration: 336, Current Total Reward: -153.0, Running Total Reward: -135.4\n",
      "Iteration: 337, Current Total Reward: -165.0, Running Total Reward: -138.36\n",
      "Iteration: 338, Current Total Reward: -86.0, Running Total Reward: -133.12\n",
      "Iteration: 339, Current Total Reward: -161.0, Running Total Reward: -135.91\n",
      "Iteration: 340, Current Total Reward: -155.0, Running Total Reward: -137.82\n",
      "Iteration: 341, Current Total Reward: -86.0, Running Total Reward: -132.64\n",
      "Iteration: 342, Current Total Reward: -153.0, Running Total Reward: -134.67\n",
      "Iteration: 343, Current Total Reward: -170.0, Running Total Reward: -138.21\n",
      "Iteration: 344, Current Total Reward: -148.0, Running Total Reward: -139.19\n",
      "Iteration: 345, Current Total Reward: -154.0, Running Total Reward: -140.67\n",
      "Iteration: 346, Current Total Reward: -156.0, Running Total Reward: -142.2\n",
      "Iteration: 347, Current Total Reward: -140.0, Running Total Reward: -141.98\n",
      "Iteration: 348, Current Total Reward: -103.0, Running Total Reward: -138.08\n",
      "Iteration: 349, Current Total Reward: -156.0, Running Total Reward: -139.87\n",
      "Iteration: 350, Current Total Reward: -152.0, Running Total Reward: -141.09\n",
      "Iteration: 351, Current Total Reward: -161.0, Running Total Reward: -143.08\n",
      "Iteration: 352, Current Total Reward: -162.0, Running Total Reward: -144.97\n",
      "Iteration: 353, Current Total Reward: -158.0, Running Total Reward: -146.27\n",
      "Iteration: 354, Current Total Reward: -155.0, Running Total Reward: -147.15\n",
      "Iteration: 355, Current Total Reward: -158.0, Running Total Reward: -148.23\n",
      "Iteration: 356, Current Total Reward: -150.0, Running Total Reward: -148.41\n",
      "Iteration: 357, Current Total Reward: -102.0, Running Total Reward: -143.77\n",
      "Iteration: 358, Current Total Reward: -150.0, Running Total Reward: -144.39\n",
      "Iteration: 359, Current Total Reward: -155.0, Running Total Reward: -145.45\n",
      "Iteration: 360, Current Total Reward: -147.0, Running Total Reward: -145.61\n",
      "Iteration: 361, Current Total Reward: -143.0, Running Total Reward: -145.35\n",
      "Iteration: 362, Current Total Reward: -164.0, Running Total Reward: -147.21\n",
      "Iteration: 363, Current Total Reward: -95.0, Running Total Reward: -141.99\n",
      "Iteration: 364, Current Total Reward: -91.0, Running Total Reward: -136.89\n",
      "Iteration: 365, Current Total Reward: -87.0, Running Total Reward: -131.9\n",
      "Iteration: 366, Current Total Reward: -151.0, Running Total Reward: -133.81\n",
      "Iteration: 367, Current Total Reward: -100.0, Running Total Reward: -130.43\n",
      "Iteration: 368, Current Total Reward: -152.0, Running Total Reward: -132.59\n",
      "Iteration: 369, Current Total Reward: -185.0, Running Total Reward: -137.83\n",
      "Iteration: 370, Current Total Reward: -86.0, Running Total Reward: -132.65\n",
      "Iteration: 371, Current Total Reward: -149.0, Running Total Reward: -134.28\n",
      "Iteration: 372, Current Total Reward: -95.0, Running Total Reward: -130.35\n",
      "Iteration: 373, Current Total Reward: -152.0, Running Total Reward: -132.52\n",
      "Iteration: 374, Current Total Reward: -164.0, Running Total Reward: -135.67\n",
      "Iteration: 375, Current Total Reward: -160.0, Running Total Reward: -138.1\n",
      "Iteration: 376, Current Total Reward: -99.0, Running Total Reward: -134.19\n",
      "Iteration: 377, Current Total Reward: -86.0, Running Total Reward: -129.37\n",
      "Iteration: 378, Current Total Reward: -151.0, Running Total Reward: -131.53\n",
      "Iteration: 379, Current Total Reward: -100.0, Running Total Reward: -128.38\n",
      "Iteration: 380, Current Total Reward: -155.0, Running Total Reward: -131.04\n",
      "Iteration: 381, Current Total Reward: -155.0, Running Total Reward: -133.44\n",
      "Iteration: 382, Current Total Reward: -166.0, Running Total Reward: -136.69\n",
      "Iteration: 383, Current Total Reward: -122.0, Running Total Reward: -135.22\n",
      "Iteration: 384, Current Total Reward: -180.0, Running Total Reward: -139.7\n",
      "Iteration: 385, Current Total Reward: -104.0, Running Total Reward: -136.13\n",
      "Iteration: 386, Current Total Reward: -152.0, Running Total Reward: -137.72\n",
      "Iteration: 387, Current Total Reward: -158.0, Running Total Reward: -139.75\n",
      "Iteration: 388, Current Total Reward: -157.0, Running Total Reward: -141.47\n",
      "Iteration: 389, Current Total Reward: -87.0, Running Total Reward: -136.03\n",
      "Iteration: 390, Current Total Reward: -175.0, Running Total Reward: -139.92\n",
      "Iteration: 391, Current Total Reward: -156.0, Running Total Reward: -141.53\n",
      "Iteration: 392, Current Total Reward: -155.0, Running Total Reward: -142.88\n",
      "Iteration: 393, Current Total Reward: -125.0, Running Total Reward: -141.09\n",
      "Iteration: 394, Current Total Reward: -122.0, Running Total Reward: -139.18\n",
      "Iteration: 395, Current Total Reward: -85.0, Running Total Reward: -133.76\n",
      "Iteration: 396, Current Total Reward: -154.0, Running Total Reward: -135.79\n",
      "Iteration: 397, Current Total Reward: -155.0, Running Total Reward: -137.71\n",
      "Iteration: 398, Current Total Reward: -92.0, Running Total Reward: -133.14\n",
      "Iteration: 399, Current Total Reward: -157.0, Running Total Reward: -135.52\n",
      "Iteration: 400, Current Total Reward: -168.0, Running Total Reward: -138.77\n",
      "Iteration: 401, Current Total Reward: -160.0, Running Total Reward: -140.89\n",
      "Iteration: 402, Current Total Reward: -155.0, Running Total Reward: -142.3\n",
      "Iteration: 403, Current Total Reward: -177.0, Running Total Reward: -145.77\n",
      "Iteration: 404, Current Total Reward: -88.0, Running Total Reward: -140.0\n",
      "Iteration: 405, Current Total Reward: -97.0, Running Total Reward: -135.7\n",
      "Iteration: 406, Current Total Reward: -162.0, Running Total Reward: -138.33\n",
      "Iteration: 407, Current Total Reward: -148.0, Running Total Reward: -139.29\n",
      "Iteration: 408, Current Total Reward: -157.0, Running Total Reward: -141.07\n",
      "Iteration: 409, Current Total Reward: -92.0, Running Total Reward: -136.16\n",
      "Iteration: 410, Current Total Reward: -145.0, Running Total Reward: -137.04\n",
      "Iteration: 411, Current Total Reward: -100.0, Running Total Reward: -133.34\n",
      "Iteration: 412, Current Total Reward: -148.0, Running Total Reward: -134.8\n",
      "Iteration: 413, Current Total Reward: -161.0, Running Total Reward: -137.42\n",
      "Iteration: 414, Current Total Reward: -161.0, Running Total Reward: -139.78\n",
      "Iteration: 415, Current Total Reward: -184.0, Running Total Reward: -144.2\n",
      "Iteration: 416, Current Total Reward: -137.0, Running Total Reward: -143.48\n",
      "Iteration: 417, Current Total Reward: -156.0, Running Total Reward: -144.73\n",
      "Iteration: 418, Current Total Reward: -157.0, Running Total Reward: -145.96\n",
      "Iteration: 419, Current Total Reward: -89.0, Running Total Reward: -140.27\n",
      "Iteration: 420, Current Total Reward: -158.0, Running Total Reward: -142.04\n",
      "Iteration: 421, Current Total Reward: -161.0, Running Total Reward: -143.93\n",
      "Iteration: 422, Current Total Reward: -150.0, Running Total Reward: -144.54\n",
      "Iteration: 423, Current Total Reward: -119.0, Running Total Reward: -141.99\n",
      "Iteration: 424, Current Total Reward: -97.0, Running Total Reward: -137.49\n",
      "Iteration: 425, Current Total Reward: -135.0, Running Total Reward: -137.24\n",
      "Iteration: 426, Current Total Reward: -155.0, Running Total Reward: -139.02\n",
      "Iteration: 427, Current Total Reward: -97.0, Running Total Reward: -134.81\n",
      "Iteration: 428, Current Total Reward: -164.0, Running Total Reward: -137.73\n",
      "Iteration: 429, Current Total Reward: -156.0, Running Total Reward: -139.56\n",
      "Iteration: 430, Current Total Reward: -101.0, Running Total Reward: -135.7\n",
      "Iteration: 431, Current Total Reward: -165.0, Running Total Reward: -138.63\n",
      "Iteration: 432, Current Total Reward: -91.0, Running Total Reward: -133.87\n",
      "Iteration: 433, Current Total Reward: -154.0, Running Total Reward: -135.88\n",
      "Iteration: 434, Current Total Reward: -114.0, Running Total Reward: -133.69\n",
      "Iteration: 435, Current Total Reward: -151.0, Running Total Reward: -135.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 436, Current Total Reward: -145.0, Running Total Reward: -136.38\n",
      "Iteration: 437, Current Total Reward: -169.0, Running Total Reward: -139.64\n",
      "Iteration: 438, Current Total Reward: -152.0, Running Total Reward: -140.88\n",
      "Iteration: 439, Current Total Reward: -154.0, Running Total Reward: -142.19\n",
      "Iteration: 440, Current Total Reward: -171.0, Running Total Reward: -145.07\n",
      "Iteration: 441, Current Total Reward: -129.0, Running Total Reward: -143.47\n",
      "Iteration: 442, Current Total Reward: -137.0, Running Total Reward: -142.82\n",
      "Iteration: 443, Current Total Reward: -113.0, Running Total Reward: -139.84\n",
      "Iteration: 444, Current Total Reward: -87.0, Running Total Reward: -134.55\n",
      "Iteration: 445, Current Total Reward: -117.0, Running Total Reward: -132.8\n",
      "Iteration: 446, Current Total Reward: -85.0, Running Total Reward: -128.02\n",
      "Iteration: 447, Current Total Reward: -118.0, Running Total Reward: -127.02\n",
      "Iteration: 448, Current Total Reward: -119.0, Running Total Reward: -126.21\n",
      "Iteration: 449, Current Total Reward: -158.0, Running Total Reward: -129.39\n",
      "Iteration: 450, Current Total Reward: -159.0, Running Total Reward: -132.35\n",
      "Iteration: 451, Current Total Reward: -162.0, Running Total Reward: -135.32\n",
      "Iteration: 452, Current Total Reward: -155.0, Running Total Reward: -137.29\n",
      "Iteration: 453, Current Total Reward: -149.0, Running Total Reward: -138.46\n",
      "Iteration: 454, Current Total Reward: -155.0, Running Total Reward: -140.11\n",
      "Iteration: 455, Current Total Reward: -132.0, Running Total Reward: -139.3\n",
      "Iteration: 456, Current Total Reward: -156.0, Running Total Reward: -140.97\n",
      "Iteration: 457, Current Total Reward: -155.0, Running Total Reward: -142.37\n",
      "Iteration: 458, Current Total Reward: -165.0, Running Total Reward: -144.64\n",
      "Iteration: 459, Current Total Reward: -161.0, Running Total Reward: -146.27\n",
      "Iteration: 460, Current Total Reward: -158.0, Running Total Reward: -147.45\n",
      "Iteration: 461, Current Total Reward: -92.0, Running Total Reward: -141.9\n",
      "Iteration: 462, Current Total Reward: -173.0, Running Total Reward: -145.01\n",
      "Iteration: 463, Current Total Reward: -157.0, Running Total Reward: -146.21\n",
      "Iteration: 464, Current Total Reward: -94.0, Running Total Reward: -140.99\n",
      "Iteration: 465, Current Total Reward: -165.0, Running Total Reward: -143.39\n",
      "Iteration: 466, Current Total Reward: -97.0, Running Total Reward: -138.75\n",
      "Iteration: 467, Current Total Reward: -92.0, Running Total Reward: -134.08\n",
      "Iteration: 468, Current Total Reward: -91.0, Running Total Reward: -129.77\n",
      "Iteration: 469, Current Total Reward: -155.0, Running Total Reward: -132.29\n",
      "Iteration: 470, Current Total Reward: -86.0, Running Total Reward: -127.66\n",
      "Iteration: 471, Current Total Reward: -90.0, Running Total Reward: -123.9\n",
      "Iteration: 472, Current Total Reward: -102.0, Running Total Reward: -121.71\n",
      "Iteration: 473, Current Total Reward: -158.0, Running Total Reward: -125.34\n",
      "Iteration: 474, Current Total Reward: -162.0, Running Total Reward: -129.0\n",
      "Iteration: 475, Current Total Reward: -165.0, Running Total Reward: -132.6\n",
      "Iteration: 476, Current Total Reward: -161.0, Running Total Reward: -135.44\n",
      "Iteration: 477, Current Total Reward: -163.0, Running Total Reward: -138.2\n",
      "Iteration: 478, Current Total Reward: -153.0, Running Total Reward: -139.68\n",
      "Iteration: 479, Current Total Reward: -95.0, Running Total Reward: -135.21\n",
      "Iteration: 480, Current Total Reward: -168.0, Running Total Reward: -138.49\n",
      "Iteration: 481, Current Total Reward: -85.0, Running Total Reward: -133.14\n",
      "Iteration: 482, Current Total Reward: -99.0, Running Total Reward: -129.73\n",
      "Iteration: 483, Current Total Reward: -90.0, Running Total Reward: -125.75\n",
      "Iteration: 484, Current Total Reward: -88.0, Running Total Reward: -121.98\n",
      "Iteration: 485, Current Total Reward: -155.0, Running Total Reward: -125.28\n",
      "Iteration: 486, Current Total Reward: -145.0, Running Total Reward: -127.25\n",
      "Iteration: 487, Current Total Reward: -159.0, Running Total Reward: -130.43\n",
      "Iteration: 488, Current Total Reward: -86.0, Running Total Reward: -125.98\n",
      "Iteration: 489, Current Total Reward: -158.0, Running Total Reward: -129.19\n",
      "Iteration: 490, Current Total Reward: -88.0, Running Total Reward: -125.07\n",
      "Iteration: 491, Current Total Reward: -90.0, Running Total Reward: -121.56\n",
      "Iteration: 492, Current Total Reward: -86.0, Running Total Reward: -118.0\n",
      "Iteration: 493, Current Total Reward: -150.0, Running Total Reward: -121.2\n",
      "Iteration: 494, Current Total Reward: -161.0, Running Total Reward: -125.18\n",
      "Iteration: 495, Current Total Reward: -161.0, Running Total Reward: -128.77\n",
      "Iteration: 496, Current Total Reward: -175.0, Running Total Reward: -133.39\n",
      "Iteration: 497, Current Total Reward: -149.0, Running Total Reward: -134.95\n",
      "Iteration: 498, Current Total Reward: -161.0, Running Total Reward: -137.55\n",
      "Iteration: 499, Current Total Reward: -85.0, Running Total Reward: -132.3\n"
     ]
    }
   ],
   "source": [
    "import gym, os\n",
    "from itertools import count, product\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config_file = \"/home/raghuram/Prateek_Codes/Final Off-Policy Experiments/MountainCar-Shaped/NAC_TD0/train_config.json\"\n",
    "\n",
    "with open(config_file) as json_file:\n",
    "    config = json.load(json_file)\n",
    "\n",
    "def video_callable(episode_number):\n",
    "    return episode_number%config['recording_frequency'] == 0\n",
    "\n",
    "env = gym.make(config['env'])\n",
    "ratio_estimation_env = gym.make(config['env'])\n",
    "\n",
    "if config['record']:\n",
    "    env = gym.wrappers.Monitor(env, config['recording_path'], force = True, video_callable=video_callable)\n",
    "\n",
    "if config['episode_length'] is not None:\n",
    "    env._max_episode_steps = config['episode_length']\n",
    "    ratio_estimation_env._max_episode_steps = config['episode_length']\n",
    "\n",
    "if config['numpy_seed'] is not None:\n",
    "    np.random.seed(config['numpy_seed'])\n",
    "\n",
    "if config['environment_seed'] is not None:\n",
    "    env.seed(config['environment_seed'])\n",
    "    ratio_estimation_env.seed(config['environment_seed'])\n",
    "\n",
    "if config['pytorch_seed'] is not None:\n",
    "    torch.manual_seed(config['pytorch_seed'])\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "actor_h_layers_sizes = config['actor']['hidden_layer_neurons']\n",
    "v_critic_h_layers_sizes = config['value_critic']['hidden_layer_neurons']\n",
    "w_h_layers_sizes = config['w']['hidden_layer_neurons']\n",
    "y_h_layers_sizes = config['y']['hidden_layer_neurons']\n",
    "gamma = config['gamma']\n",
    "lr_A = config['actor']['learning_rate']\n",
    "lr_A_C = config['advantage_critic']['learning_rate']\n",
    "lr_V_C = config['value_critic']['learning_rate']\n",
    "lr_W = config['w']['learning_rate']\n",
    "lr_Y = config['y']['learning_rate']\n",
    "load_A = config['actor']['load']\n",
    "load_A_C = config['advantage_critic']['load']\n",
    "load_V_C = config['value_critic']['load']\n",
    "load_W = config['w']['load']\n",
    "load_Y = config['y']['load']\n",
    "random_behaviour = config['random_behaviour']\n",
    "iterations = config['iterations']\n",
    "estimation_samples = config['estimation_samples']\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_size, h_layers_sizes, output_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.h_layers_sizes = h_layers_sizes\n",
    "        self.all_layers_sizes = [input_size] + h_layers_sizes + [output_size]\n",
    "        self.output_size = output_size\n",
    "        self.linears = nn.ModuleList([nn.Linear(self.all_layers_sizes[i], self.all_layers_sizes[i+1], bias=False) for i in range(len(self.all_layers_sizes)-1)])\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = torch.tanh(self.linears[0](state))\n",
    "        for i in range(1,len(self.linears)-1):\n",
    "            output = torch.tanh(self.linears[i](output))\n",
    "        output = self.linears[-1](output)\n",
    "        distribution = Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "class V_Critic(nn.Module):\n",
    "    def __init__(self, input_size, h_layers_sizes, output_size):\n",
    "        super(V_Critic, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.h_layers_sizes = h_layers_sizes\n",
    "        self.all_layers_sizes = [input_size] + h_layers_sizes + [output_size]\n",
    "        self.output_size = output_size\n",
    "        self.linears = nn.ModuleList([nn.Linear(self.all_layers_sizes[i], self.all_layers_sizes[i+1]) for i in range(len(self.all_layers_sizes)-1)])\n",
    "\n",
    "    def forward(self, state):\n",
    "        value = F.relu(self.linears[0](state))\n",
    "        for i in range(1,len(self.linears)-1):\n",
    "            value = F.relu(self.linears[i](value))\n",
    "        value = self.linears[-1](value)\n",
    "        return value\n",
    "\n",
    "class A_Critic(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(A_Critic, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.linear1 = nn.Linear(self.input_size, self.output_size, bias=False)\n",
    "\n",
    "    def forward(self, state):\n",
    "        value = self.linear1(state)\n",
    "        return value\n",
    "\n",
    "class W(nn.Module):\n",
    "    def __init__(self, input_size, h_layers_sizes, output_size):\n",
    "        super(W, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.h_layers_sizes = h_layers_sizes\n",
    "        self.all_layers_sizes = [input_size] + h_layers_sizes + [output_size]\n",
    "        self.output_size = output_size\n",
    "        self.linears = nn.ModuleList([nn.Linear(self.all_layers_sizes[i], self.all_layers_sizes[i+1]) for i in range(len(self.all_layers_sizes)-1)])\n",
    "\n",
    "    def forward(self, state):\n",
    "        value = F.relu(self.linears[0](state))\n",
    "        for i in range(1,len(self.linears)-1):\n",
    "            value = F.relu(self.linears[i](value))\n",
    "        value = torch.exp(self.linears[-1](value))\n",
    "        return value\n",
    "\n",
    "class Y(nn.Module):\n",
    "    def __init__(self, input_size, h_layers_sizes, output_size):\n",
    "        super(Y, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.h_layers_sizes = h_layers_sizes\n",
    "        self.all_layers_sizes = [input_size] + h_layers_sizes + [output_size]\n",
    "        self.output_size = output_size\n",
    "        self.linears = nn.ModuleList([nn.Linear(self.all_layers_sizes[i], self.all_layers_sizes[i+1]) for i in range(len(self.all_layers_sizes)-1)])\n",
    "\n",
    "    def forward(self, state):\n",
    "        value = F.relu(self.linears[0](state))\n",
    "        for i in range(1,len(self.linears)-1):\n",
    "            value = F.relu(self.linears[i](value))\n",
    "        value = torch.exp(self.linears[-1](value))\n",
    "        return value\n",
    "\n",
    "def kernel(state1, state2, l = 1):\n",
    "    d = torch.norm(state1 - state2)\n",
    "    return torch.exp(-((d**2)/(2*(l**2))))\n",
    "\n",
    "def estimate_W(target_policy, w, optimizerW, random_behaviour = True, behaviour_policy = None, iterations = 5000, samples = 1000):\n",
    "\n",
    "    for iter in range(iterations):\n",
    "        w_state1_list = []\n",
    "        w_state2_list = []\n",
    "        w_next_state1_list = []\n",
    "        w_next_state2_list = []\n",
    "        beta1_list = []\n",
    "        beta2_list = []\n",
    "        kernel_value_list = []\n",
    "        z_w_state = 0\n",
    "\n",
    "        training_data = []\n",
    "        initial_state = ratio_estimation_env.reset()\n",
    "        initial_state = torch.FloatTensor(initial_state).to(device)\n",
    "        state = initial_state\n",
    "\n",
    "        for i in range(samples):\n",
    "            \n",
    "            if random_behaviour == False:\n",
    "                dist_behaviour = behaviour_policy(state)\n",
    "                action = dist_behaviour.sample()\n",
    "            else:\n",
    "                action = torch.randint(0, action_size, (1,)).to(device)\n",
    "                action = torch.squeeze(action)\n",
    "\n",
    "            next_state, reward, done, _ = ratio_estimation_env.step(action.cpu().numpy())\n",
    "\n",
    "            next_state = torch.FloatTensor(next_state).to(device)\n",
    "\n",
    "            dist_target = target_policy(state)\n",
    "\n",
    "            if random_behaviour == False:\n",
    "                beta = (dist_target.probs[action]/dist_behaviour.probs[action]).detach()\n",
    "            else:\n",
    "                beta = dist_target.probs[action].detach()*action_size\n",
    "            \n",
    "            training_data.append([state,beta,next_state])\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        batch = [[None, None, initial_state]]\n",
    "\n",
    "        for i in range(len(training_data)):\n",
    "            d = np.random.uniform()\n",
    "            if d < gamma**(i+1):\n",
    "                batch.append(training_data[i])\n",
    "\n",
    "        pairs = list(product(batch, repeat=2))\n",
    "\n",
    "        for pair in pairs:\n",
    "            sample1 = pair[0]\n",
    "            sample2 = pair[1]\n",
    "\n",
    "            if sample1[0] != None:\n",
    "                w_state1 = w(sample1[0])\n",
    "            else:\n",
    "                w_state1 = None\n",
    "\n",
    "            if sample2[0] != None:\n",
    "                w_state2 = w(sample2[0])\n",
    "            else:\n",
    "                w_state2 = None\n",
    "\n",
    "            beta1 = sample1[1]\n",
    "            beta2 = sample2[1]\n",
    "\n",
    "            w_next_state1 = w(sample1[2])\n",
    "            w_next_state2 = w(sample2[2])\n",
    "\n",
    "            kernel_value = kernel(sample1[2], sample2[2])\n",
    "\n",
    "            w_state1_list.append(w_state1)\n",
    "            w_state2_list.append(w_state2)\n",
    "            w_next_state1_list.append(w_next_state1)\n",
    "            w_next_state2_list.append(w_next_state2)\n",
    "            beta1_list.append(beta1)\n",
    "            beta2_list.append(beta2)\n",
    "            kernel_value_list.append(kernel_value)\n",
    "\n",
    "        for sample in batch[1:]:\n",
    "            w_state = w(sample[0])\n",
    "            z_w_state += w_state\n",
    "\n",
    "        z_w_state /= len(batch)\n",
    "\n",
    "        w_loss = 0\n",
    "\n",
    "        for i in range(len(pairs)):\n",
    "            if w_state1_list[i] == None and w_state2_list[i] == None:\n",
    "                w_loss += (1 - (w_next_state1_list[i]/z_w_state))*(1 - (w_next_state2_list[i]/z_w_state))*kernel_value_list[i]\n",
    "\n",
    "            elif w_state1_list[i] == None:\n",
    "                w_loss += (1 - (w_next_state1_list[i]/z_w_state))*(beta2_list[i]*(w_state2_list[i]/z_w_state) - (w_next_state2_list[i]/z_w_state))*kernel_value_list[i]\n",
    "\n",
    "            elif w_state2_list[i] == None:\n",
    "                w_loss += (beta1_list[i]*(w_state1_list[i]/z_w_state) - (w_next_state1_list[i]/z_w_state))*(1 - (w_next_state2_list[i]/z_w_state))*kernel_value_list[i]\n",
    "\n",
    "            else:\n",
    "                w_loss += (beta1_list[i]*(w_state1_list[i]/z_w_state) - (w_next_state1_list[i]/z_w_state))*(beta2_list[i]*(w_state2_list[i]/z_w_state) - (w_next_state2_list[i]/z_w_state))*kernel_value_list[i]\n",
    "\n",
    "        w_loss /= len(batch)\n",
    "        optimizerW.zero_grad()\n",
    "        w_loss.backward()\n",
    "        optimizerW.step()\n",
    "        optimizerW.zero_grad()\n",
    "\n",
    "def estimate_Y(target_policy, y, optimizerY, random_behaviour = True, behaviour_policy = None, iterations = 5000, samples = 1000):\n",
    "\n",
    "    for iter in range(iterations):\n",
    "        y_state1_list = []\n",
    "        y_state2_list = []\n",
    "        y_next_state1_list = []\n",
    "        y_next_state2_list = []\n",
    "        beta1_list = []\n",
    "        beta2_list = []\n",
    "        kernel_value_list = []\n",
    "        z_y_state = 0\n",
    "\n",
    "        training_data = []\n",
    "        state = ratio_estimation_env.reset()\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "\n",
    "        for i in range(samples):\n",
    "            \n",
    "            if random_behaviour == False:\n",
    "                dist_behaviour = behaviour_policy(state)\n",
    "                action = dist_behaviour.sample()\n",
    "            else:\n",
    "                action = torch.randint(0, action_size, (1,)).to(device)\n",
    "                action = torch.squeeze(action)\n",
    "\n",
    "            next_state, reward, done, _ = ratio_estimation_env.step(action.cpu().numpy())\n",
    "\n",
    "            next_state = torch.FloatTensor(next_state).to(device)\n",
    "\n",
    "            dist_target = target_policy(state)\n",
    "\n",
    "            if random_behaviour == False:\n",
    "                beta = (dist_target.probs[action]/dist_behaviour.probs[action]).detach()\n",
    "            else:\n",
    "                beta = dist_target.probs[action].detach()*action_size\n",
    "            \n",
    "            training_data.append([state,beta,next_state])\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        pairs = list(product(training_data, repeat=2))\n",
    "\n",
    "        for pair in pairs:\n",
    "            sample1 = pair[0]\n",
    "            sample2 = pair[1]\n",
    "\n",
    "            y_state1 = y(sample1[0])\n",
    "            y_state2 = y(sample2[0])\n",
    "\n",
    "            beta1 = sample1[1]\n",
    "            beta2 = sample2[1]\n",
    "\n",
    "            y_next_state1 = y(sample1[2])\n",
    "            y_next_state2 = y(sample2[2])\n",
    "\n",
    "            kernel_value = kernel(sample1[2], sample2[2])\n",
    "\n",
    "            y_state1_list.append(y_state1)\n",
    "            y_state2_list.append(y_state2)\n",
    "            y_next_state1_list.append(y_next_state1)\n",
    "            y_next_state2_list.append(y_next_state2)\n",
    "            beta1_list.append(beta1)\n",
    "            beta2_list.append(beta2)\n",
    "            kernel_value_list.append(kernel_value)\n",
    "\n",
    "        for sample in training_data:\n",
    "            y_state = y(sample[0])\n",
    "            z_y_state += y_state\n",
    "\n",
    "        z_y_state /= len(training_data)\n",
    "\n",
    "        y_loss = 0\n",
    "\n",
    "        for i in range(len(pairs)):\n",
    "            y_loss += (beta1_list[i]*(y_state1_list[i]/z_y_state) - (y_next_state1_list[i]/z_y_state))*(beta2_list[i]*(y_state2_list[i]/z_y_state) - (y_next_state2_list[i]/z_y_state))*kernel_value_list[i]\n",
    "\n",
    "        y_loss /= len(training_data)\n",
    "        optimizerY.zero_grad()\n",
    "        y_loss.backward()\n",
    "        optimizerY.step()\n",
    "        optimizerY.zero_grad()\n",
    "\n",
    "def lr_scheduler(optimizerA, optimizerA_C, optimizerV_C, total_reward):\n",
    "    for schedule in config['learning_rate_scheduler']['schedule']:\n",
    "        if total_reward >= schedule[0][0] and total_reward < schedule[0][1]:\n",
    "            optimizerA.param_groups[0]['lr'] = schedule[1]['lr_A']\n",
    "            optimizerA_C.param_groups[0]['lr'] = schedule[1]['lr_A_C']\n",
    "            optimizerV_C.param_groups[0]['lr'] = schedule[1]['lr_V_C']\n",
    "\n",
    "def evaluate_policy(actor):\n",
    "    state = env.reset()\n",
    "    state = torch.FloatTensor(state).to(device)\n",
    "    total_reward = 0\n",
    "\n",
    "    for i in count():\n",
    "        if config['render']:\n",
    "            env.render()\n",
    "\n",
    "        dist = actor(state)\n",
    "        action = dist.sample()\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "        total_reward += reward\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "def trainIters(actor, v_critic, a_critic, w, y, random_behaviour = True, behaviour_policy = None, n_iters = 5000):\n",
    "    optimizerA = optim.SGD(actor.parameters(), lr=lr_A)\n",
    "    optimizerA_C = optim.SGD(a_critic.parameters(), lr=lr_A_C)\n",
    "    optimizerV_C = optim.Adam(v_critic.parameters(), lr=lr_V_C)\n",
    "    optimizerW = optim.Adam(w.parameters(), lr=lr_W)\n",
    "    optimizerY = optim.Adam(y.parameters(), lr=lr_Y)\n",
    "    running_total_reward = 0\n",
    "    max_running_total_reward = -float('inf')\n",
    "    reward_list = []\n",
    "\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        estimate_W(actor, w, optimizerW, random_behaviour, behaviour_policy, iterations = 1, samples = estimation_samples)\n",
    "        estimate_Y(actor, y, optimizerY, random_behaviour, behaviour_policy, iterations = 1, samples = estimation_samples)\n",
    "        \n",
    "        for i in count():\n",
    "            \n",
    "            if random_behaviour == False:\n",
    "                dist_behaviour = behaviour_policy(state)\n",
    "                action = dist_behaviour.sample()\n",
    "            else:\n",
    "                action = torch.randint(0, action_size, (1,)).to(device)\n",
    "                action = torch.squeeze(action)\n",
    "            \n",
    "            dist_target = actor(state)\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "            \n",
    "            if next_state[0] > 0.1 and next_state[0] < 0.4:\n",
    "                reward += 10\n",
    "            elif next_state[0] >= 0.4 and next_state[0] < 0.6:\n",
    "                reward += 20\n",
    "            if done == True and i < config['episode_length']-1:\n",
    "                reward += 1000\n",
    "\n",
    "            if random_behaviour == False:\n",
    "                beta = (dist_target.probs[action]/dist_behaviour.probs[action]).detach()\n",
    "            else:\n",
    "                beta = dist_target.probs[action].detach()*action_size\n",
    "                \n",
    "            log_prob = dist_target.log_prob(action).unsqueeze(0)\n",
    "            pseudo_loss = log_prob\n",
    "            pseudo_loss.backward(retain_graph=True)\n",
    "\n",
    "            compatible_features = torch.FloatTensor([]).to(device)\n",
    "            for params in actor.parameters():\n",
    "                compatible_features = torch.cat((compatible_features, torch.flatten(params.grad)))\n",
    "\n",
    "            v_value = v_critic(state)\n",
    "            a_value = a_critic(compatible_features)\n",
    "\n",
    "            next_state = torch.FloatTensor(next_state).to(device)\n",
    "            next_v_value = v_critic(next_state)\n",
    "\n",
    "            if done:\n",
    "                delta = reward - v_value\n",
    "                v_critic_loss = delta.pow(2)\n",
    "                optimizerV_C.zero_grad()\n",
    "                v_critic_loss.backward()\n",
    "                y_ratio = y(state).detach()\n",
    "                for j in range(len(v_critic.all_layers_sizes)-1):\n",
    "                    v_critic.linears[j].weight.grad = y_ratio*beta*v_critic.linears[j].weight.grad\n",
    "                optimizerV_C.step()\n",
    "                optimizerV_C.zero_grad()\n",
    "\n",
    "                v_value = v_critic(state)\n",
    "                next_v_value = v_critic(next_state)\n",
    "                delta = (reward - v_value).detach()\n",
    "                error = delta - a_value\n",
    "                a_critic_loss = error.pow(2)\n",
    "                optimizerA_C.zero_grad()\n",
    "                a_critic_loss.backward()\n",
    "                w_ratio = w(state).detach()\n",
    "                a_critic.linear1.weight.grad = w_ratio*beta*a_critic.linear1.weight.grad\n",
    "                optimizerA_C.step()\n",
    "                optimizerA_C.zero_grad()\n",
    "\n",
    "                optimizerA.zero_grad()\n",
    "                critic_weights = a_critic.linear1.weight.detach().clone()\n",
    "                start = 0\n",
    "                for j in range(0,len(actor.all_layers_sizes)-1):\n",
    "                    actor.linears[j].weight.grad = -1*torch.reshape(torch.narrow(critic_weights, 1, start, actor.all_layers_sizes[j]*actor.all_layers_sizes[j+1]), (actor.all_layers_sizes[j+1], actor.all_layers_sizes[j]))\n",
    "                    start += actor.all_layers_sizes[j]*actor.all_layers_sizes[j+1]\n",
    "                optimizerA.step()\n",
    "                optimizerA.zero_grad()\n",
    "\n",
    "                total_reward = evaluate_policy(actor)\n",
    "\n",
    "                running_total_reward = total_reward if running_total_reward == 0 else running_total_reward * 0.9 + total_reward * 0.1\n",
    "                print('Iteration: {}, Current Total Reward: {}, Running Total Reward: {}'.format(iter, total_reward, round(running_total_reward,2)))\n",
    "\n",
    "                reward_list.append(running_total_reward)\n",
    "\n",
    "                if max_running_total_reward <= running_total_reward:\n",
    "                    torch.save(actor, config['actor']['final_save_path'])\n",
    "                    torch.save(v_critic, config['value_critic']['final_save_path'])\n",
    "                    torch.save(a_critic, config['advantage_critic']['final_save_path'])\n",
    "                    torch.save(a_critic, config['w']['final_save_path'])\n",
    "                    torch.save(a_critic, config['y']['final_save_path'])\n",
    "                    max_running_total_reward = running_total_reward\n",
    "\n",
    "                if config['learning_rate_scheduler']['required']:\n",
    "                    lr_scheduler(optimizerA, optimizerA_C, optimizerV_C, max_running_total_reward)\n",
    "\n",
    "                break\n",
    "            else:\n",
    "                delta = reward + gamma * next_v_value.detach() - v_value\n",
    "                v_critic_loss = delta.pow(2)\n",
    "                optimizerV_C.zero_grad()\n",
    "                v_critic_loss.backward()\n",
    "                y_ratio = y(state).detach()\n",
    "                for j in range(len(v_critic.all_layers_sizes)-1):\n",
    "                    v_critic.linears[j].weight.grad = y_ratio*beta*v_critic.linears[j].weight.grad\n",
    "                optimizerV_C.step()\n",
    "                optimizerV_C.zero_grad()\n",
    "\n",
    "                v_value = v_critic(state)\n",
    "                next_v_value = v_critic(next_state)\n",
    "                delta = (reward + gamma * next_v_value - v_value).detach()\n",
    "                error = delta - a_value\n",
    "                a_critic_loss = error.pow(2)\n",
    "                optimizerA_C.zero_grad()\n",
    "                a_critic_loss.backward()\n",
    "                w_ratio = w(state).detach()\n",
    "                a_critic.linear1.weight.grad = w_ratio*beta*a_critic.linear1.weight.grad\n",
    "                optimizerA_C.step()\n",
    "                optimizerA_C.zero_grad()\n",
    "\n",
    "                optimizerA.zero_grad()\n",
    "                critic_weights = a_critic.linear1.weight.detach().clone()\n",
    "                start = 0\n",
    "                for j in range(0,len(actor.all_layers_sizes)-1):\n",
    "                    actor.linears[j].weight.grad = -1*torch.reshape(torch.narrow(critic_weights, 1, start, actor.all_layers_sizes[j]*actor.all_layers_sizes[j+1]), (actor.all_layers_sizes[j+1], actor.all_layers_sizes[j]))\n",
    "                    start += actor.all_layers_sizes[j]*actor.all_layers_sizes[j+1]\n",
    "                optimizerA.step()\n",
    "                optimizerA.zero_grad()\n",
    "\n",
    "                state = next_state\n",
    "        \n",
    "    env.close()\n",
    "    ratio_estimation_env.close()\n",
    "    with open(config['rewards_path'], 'w') as fp:\n",
    "        json.dump(reward_list, fp, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if load_A:\n",
    "        path_A = config['actor']['load_path']\n",
    "        actor = torch.load(path_A).to(device)\n",
    "        print('Actor Model loaded')\n",
    "    else:\n",
    "        actor = Actor(state_size, actor_h_layers_sizes, action_size).to(device)\n",
    "        torch.save(actor, config['actor']['initial_save_path'])\n",
    "    \n",
    "    if load_V_C:\n",
    "        path_V_C = config['value_critic']['load_path']\n",
    "        v_critic = torch.load(path_V_C).to(device)\n",
    "        print('Value Critic Model loaded')\n",
    "    else:    \n",
    "        v_critic = V_Critic(state_size, v_critic_h_layers_sizes, 1).to(device)\n",
    "        torch.save(v_critic, config['value_critic']['initial_save_path'])\n",
    "\n",
    "    if load_A_C:\n",
    "        path_A_C = config['advantage_critic']['load_path']\n",
    "        a_critic = torch.load(path_A_C).to(device)\n",
    "        print('Advantage Critic Model loaded')\n",
    "    else:\n",
    "        a_critic_input_size = state_size*actor_h_layers_sizes[0]\n",
    "        for i in range(len(actor_h_layers_sizes) - 1):\n",
    "            a_critic_input_size += actor_h_layers_sizes[i]*actor_h_layers_sizes[i+1]\n",
    "        a_critic_input_size += actor_h_layers_sizes[-1]*action_size\n",
    "        a_critic = A_Critic(a_critic_input_size, 1).to(device)\n",
    "        torch.save(a_critic, config['advantage_critic']['initial_save_path'])\n",
    "\n",
    "    if load_W:\n",
    "        path_W = config['w']['load_path']\n",
    "        w = torch.load(path_W).to(device)\n",
    "        print('W Model loaded')\n",
    "    else:    \n",
    "        w = W(state_size, w_h_layers_sizes, 1).to(device)\n",
    "        torch.save(w, config['w']['initial_save_path'])\n",
    "\n",
    "    if load_Y:\n",
    "        path_Y = config['y']['load_path']\n",
    "        y = torch.load(path_Y).to(device)\n",
    "        print('Y Model loaded')\n",
    "    else:    \n",
    "        y = Y(state_size, y_h_layers_sizes, 1).to(device)\n",
    "        torch.save(y, config['y']['initial_save_path'])\n",
    "\n",
    "    if random_behaviour:\n",
    "        behaviour_policy = None\n",
    "    else:\n",
    "        behaviour_policy = torch.load(config['behaviour_policy_path']).to(device)\n",
    "        print('Behaviour Policy loaded')\n",
    "\n",
    "    trainIters(actor, v_critic, a_critic, w, y, random_behaviour, behaviour_policy, n_iters=iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
